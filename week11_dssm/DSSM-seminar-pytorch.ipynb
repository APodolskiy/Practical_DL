{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This seminar__ teaches you about metric learning for NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford question answering dataset (SQuAD)\n",
    "\n",
    "_this seminar is based on original notebook by [Oleg Vasilev](https://github.com/Omrigan/)_\n",
    "\n",
    "Today we are going to work with a popular NLP dataset.\n",
    "\n",
    "Here is the description of the original problem:\n",
    "\n",
    "```\n",
    "Stanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets.\n",
    "```\n",
    "\n",
    "\n",
    "We are not going to solve it :) Instead we will try to answer the question in a different way: given the question, we will find a **sentence** containing the answer, but not within the context, but in a **whole databank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" не является внутренней или внешней\n",
      "командой, исполняемой программой или пакетным файлом.\n"
     ]
    }
   ],
   "source": [
    "# download the data\n",
    "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'qas': [{'answers': [{'answer_start': 515,\n",
       "     'text': 'Saint Bernadette Soubirous'}],\n",
       "   'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       "   'id': '5733be284776f41900661182'},\n",
       "  {'answers': [{'answer_start': 188, 'text': 'a copper statue of Christ'}],\n",
       "   'question': 'What is in front of the Notre Dame Main Building?',\n",
       "   'id': '5733be284776f4190066117f'},\n",
       "  {'answers': [{'answer_start': 279, 'text': 'the Main Building'}],\n",
       "   'question': 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?',\n",
       "   'id': '5733be284776f41900661180'},\n",
       "  {'answers': [{'answer_start': 381,\n",
       "     'text': 'a Marian place of prayer and reflection'}],\n",
       "   'question': 'What is the Grotto at Notre Dame?',\n",
       "   'id': '5733be284776f41900661181'},\n",
       "  {'answers': [{'answer_start': 92,\n",
       "     'text': 'a golden statue of the Virgin Mary'}],\n",
       "   'question': 'What sits on top of the Main Building at Notre Dame?',\n",
       "   'id': '5733be284776f4190066117e'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][0]['paragraphs'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NLP part\n",
    "\n",
    "The code here is very similar to `week10/`: preprocess text into tokens, create dictionaries, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de1b8a4407b475dad80edbd16159dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=442), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+|\\d+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenize(value):\n",
    "    return tokenizer.tokenize(value.lower())\n",
    "\n",
    "for q in tqdm.tqdm_notebook(data['data']):\n",
    "    for p in q['paragraphs']:\n",
    "        token_counts.update(tokenize(p['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 4\n",
    "\n",
    "tokens = [w for w, c in token_counts.items() if c > min_count] \n",
    "tokens = [\"_PAD_\", \"_UNK_\"] + tokens\n",
    "\n",
    "token_to_id = {t : i for i, t in enumerate(tokens)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert token_to_id['me'] != token_to_id['woods']\n",
    "assert token_to_id[tokens[42]]==42\n",
    "assert len(token_to_id)==len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ix = token_to_id[\"_PAD_\"]\n",
    "UNK_ix = token_to_id['_UNK_']\n",
    "\n",
    "#good old as_matrix for the third time\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    if isinstance(sequences[0], (str, bytes)):\n",
    "        sequences = [tokenize(s) for s in sequences]\n",
    "        \n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences), max_len), dtype='int32') + PAD_ix\n",
    "    for i, seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_ix) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11162   849     1 12434  2799 19196     0     0]\n",
      " [ 1030   311     3  5523    35   164  2605 20297]]\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "test = as_matrix([\"Definitely, thOsE tokens areN'T LowerCASE!!\", \"I'm the monument to all your sins.\"])\n",
    "print(test)\n",
    "assert test.shape==(2,8)\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def build_dataset(train_data):\n",
    "    '''Takes SQuAD data\n",
    "    Returns a list of tuples - a set of pairs (q, a_+)\n",
    "    '''\n",
    "    dataset = []\n",
    "    for row in tqdm.tqdm_notebook(train_data):\n",
    "        for paragraph in row['paragraphs']:\n",
    "            offsets = []\n",
    "            curent_index = 0\n",
    "            for sent in sent_tokenize(paragraph['context']):\n",
    "                curent_index+=len(sent)+2\n",
    "                offsets.append((curent_index, sent))\n",
    "                \n",
    "            for qa in paragraph['qas']:\n",
    "                question, answer = qa['question'], qa['answers'][0]\n",
    "                \n",
    "                #find a sentence that contains an answer\n",
    "                for offset, sent in offsets:\n",
    "                    if answer['answer_start'] < offset:\n",
    "                        dataset.append((question, sent))\n",
    "                        break\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e113f76e6c1f450b9e08caf716f862d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=397), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bf5bb9f5b14fa587ad44b6f45d4982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=45), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(data['data'], test_size=0.1)\n",
    "\n",
    "train_data = build_dataset(train_data)\n",
    "val_data = build_dataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Of what is Hinduism a combination?\n",
      "A: The history of India includes the prehistoric settlements and societies in the Indian subcontinent; the blending of the Indus Valley Civilization and Indo-Aryan culture into the Vedic Civilization; the development of Hinduism as a synthesis of various Indian cultures and traditions; the rise of the Śramaṇa movement; the decline of Śrauta sacrifices and the birth of the initiatory traditions of Jainism, Buddhism, Shaivism, Vaishnavism and Shaktism; the onset of a succession of powerful dynasties and empires for more than two millennia throughout various geographic areas of the subcontinent, including the growth of Muslim dynasties during the Medieval period intertwined with Hindu powers; the advent of European traders resulting in the establishment of the British rule; and the subsequent independence movement that led to the Partition of India and the creation of the Republic of India.\n",
      "\n",
      "Q: What was the first major civilization in South Asia?\n",
      "A: The Indus Valley Civilization which spread and flourished in the northwestern part of the Indian subcontinent from c. 3200 to 1300 BCE, was the first major civilization in South Asia.\n",
      "\n",
      "Q: What was the foremost dynasty of the the Golden Age period? \n",
      "A: Various parts of India were ruled by numerous dynasties for the next 1,500 years, among which the Gupta Empire stands out.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 18, 6):\n",
    "    print(\"Q: %s\\nA: %s\\n\" % val_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model\n",
    "\n",
    "Any self-respecting DSSM must have one or several vectorizers. In our case,\n",
    "* Context vectorizer\n",
    "* Answer vectorizer\n",
    "\n",
    "It is perfectly legal to share some layers between them, but make sure they are at least a little different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class GlobalMaxPooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.max(dim=self.dim)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we might as well create a global embedding layer here\n",
    "\n",
    "GLOBAL_EMB = nn.Embedding(len(tokens), 64, padding_idx=PAD_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionVectorizer(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64, use_global_emb=True):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for questions.\n",
    "        Use any combination of layers you want to encode a variable-length input \n",
    "        to a fixed-size output vector\n",
    "        \n",
    "        If use_global_emb is True, use GLOBAL_EMB as your embedding layer\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        if use_global_emb:\n",
    "            self.emb = GLOBAL_EMB\n",
    "        else:\n",
    "            self.emb = nn.Embedding(n_tokens, 64, padding_idx=PAD_ix)\n",
    "            \n",
    "        self.conv = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = GlobalMaxPooling()\n",
    "        self.dense = nn.Linear(128, out_size)\n",
    "        \n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        h = torch.transpose(self.emb(text_ix), 2, 1)\n",
    "        h = F.relu(self.pool(self.conv(h)))\n",
    "        return self.dense(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerVectorizer(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64, use_global_emb=True):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for answers.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \n",
    "        If use_global_emb is True, use GLOBAL_EMB as your embedding layer\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        if use_global_emb:\n",
    "            self.emb = GLOBAL_EMB\n",
    "        else:\n",
    "            self.emb = nn.Embedding(n_tokens, 64, padding_idx=PAD_ix)\n",
    "            \n",
    "        self.conv = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = GlobalMaxPooling()\n",
    "        self.dense = nn.Linear(128, out_size)\n",
    "        \n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        h = torch.transpose(self.emb(text_ix), 2, 1)\n",
    "        h = F.relu(self.pool(self.conv(h)))\n",
    "        return self.dense(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing QuestionVectorizer ...\n",
      "Seems fine\n",
      "Testing AnswerVectorizer ...\n",
      "Seems fine\n"
     ]
    }
   ],
   "source": [
    "for vectorizer in [QuestionVectorizer(out_size=100), AnswerVectorizer(out_size=100)]:\n",
    "    print(\"Testing %s ...\" % vectorizer.__class__.__name__)\n",
    "    dummy_x = Variable(torch.LongTensor(test))\n",
    "    dummy_v = vectorizer(dummy_x)\n",
    "\n",
    "    assert isinstance(dummy_v, Variable)\n",
    "    assert tuple(dummy_v.shape) == (dummy_x.shape[0], 100)\n",
    "\n",
    "    del vectorizer\n",
    "    print(\"Seems fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "question_vectorizer = QuestionVectorizer()\n",
    "answer_vectorizer = AnswerVectorizer()\n",
    "\n",
    "opt = torch.optim.Adam(chain(question_vectorizer.parameters(),\n",
    "                             answer_vectorizer.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a single `encode`, but with different weights. You can use different encode for anchor and negatives/positives.\n",
    "\n",
    "Negative sampling can be either `in-graph` or `out-graph`. We start with out-graph. In the home assignment you are going to use in-graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data, batch_size=None, replace=False, volatile=False, max_len=None):\n",
    "    \"\"\" Samples training/validation batch with random negatives \"\"\"\n",
    "    if batch_size is not None:\n",
    "        batch_ix = np.random.choice(len(data), batch_size, replace=replace)\n",
    "        negative_ix = np.random.choice(len(data), batch_size, replace=True)\n",
    "    else:\n",
    "        batch_ix = range(len(data))\n",
    "        negative_ix = np.random.permutation(np.arange(len(data)))\n",
    "\n",
    "    \n",
    "    anchors, positives = zip(*[data[i] for i in batch_ix])\n",
    "    \n",
    "    # sample random rows as negatives.\n",
    "    # Note: you can do better by sampling \"hard\" negatives\n",
    "    negatives = [data[i][1] for i in negative_ix]\n",
    "    \n",
    "    anchors, positives, negatives = map(lambda x: Variable(torch.LongTensor(as_matrix(x, max_len=max_len)),\n",
    "                                                           volatile=volatile), \n",
    "                                        [anchors, positives, negatives])\n",
    "    return anchors, positives, negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      "tensor([[  1877,   3309,     18,      3,   2025,   3212,    719,      6,\n",
      "            659,   2535,  14236,    117,   1165,     22,      1,     22,\n",
      "           7541,     35,      3,  15979,  14236],\n",
      "        [  1877,    152,      3,   1051,     18,      3,  14490,     58,\n",
      "            298,   6087,    112,      3,   1589,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0]])\n",
      "A+:\n",
      "tensor([[   112,    777,    547,   1382,      3,  22898,   1230,    130,\n",
      "           5786,    126,      3,  22722,     35,   6791,      6,   2535,\n",
      "          14236,   2579,      1,    659,  15267,     22,     42,     18,\n",
      "              3,  15979,  14236,   2579,      1,     24,     52,    540,\n",
      "            659,  15267,    188,    719,    117,   1165,    869,   1516,\n",
      "           1843,     18,      3,   3212,   1165,    126,      3,   6617,\n",
      "          15979,  14236],\n",
      "        [    22,      3,    347,   7218,      3,   1589,     18,      3,\n",
      "           2506,   5004,   3292,    230,     18,      3,  14490,     18,\n",
      "              1,    287,   1750,      1,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0]])\n",
      "A-:\n",
      "tensor([[     3,   9565,   9116,    152,      3,   7394,     18,   5765,\n",
      "             22,      3,   1115,    463,   2224,    188,  17431,      3,\n",
      "          18659,     24,      1,    147,     18,    198,      1,     24,\n",
      "           4960,    113,     24,  11004,      3,   5740,     18,   4570,\n",
      "           2335,     24,  10275],\n",
      "        [   342,    295,    291,   5711,    254,   6050,     35,   7212,\n",
      "           5628,     18,      3,    355,   1722,   2456,    680,   1178,\n",
      "          17157,   1491,      3,    807,     12,   3212,    216,  12110,\n",
      "           5125,     35,      3,   3169,  15560,    646,     22,   1310,\n",
      "           5051,      0,      0]])\n"
     ]
    }
   ],
   "source": [
    "_dummy_anchors, _dummy_positives, _dummy_negatives = generate_batch(train_data, 2)\n",
    "\n",
    "print(\"Q:\")\n",
    "print(_dummy_anchors)\n",
    "print(\"A+:\")\n",
    "print(_dummy_positives)\n",
    "print(\"A-:\")\n",
    "print(_dummy_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(anchors, positives, negatives, delta=1):\n",
    "    \"\"\" \n",
    "    Compute the triplet loss:\n",
    "    \n",
    "    max(0, delta + sim(anchors, negatives) - sim(anchors, positives))\n",
    "    \n",
    "    where sim is a dot-product between vectorized inputs\n",
    "    \n",
    "    \"\"\"\n",
    "    v_a = question_vectorizer(anchors)\n",
    "    v_p = answer_vectorizer(positives)\n",
    "    v_n = answer_vectorizer(negatives)\n",
    "    sim_n = (v_a * v_n).sum(dim = 1)\n",
    "    sim_p = (v_a * v_p).sum(dim = 1)\n",
    "    return F.relu(delta + sim_n - sim_p).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(anchors, positives, negatives, delta=1):\n",
    "    \"\"\"\n",
    "    Compute the probability (ratio) at which sim(anchors, negatives) is greater than sim(anchors, positives)\n",
    "    \"\"\"\n",
    "    v_a = question_vectorizer(anchors)\n",
    "    v_p = answer_vectorizer(positives)\n",
    "    v_n = answer_vectorizer(negatives)\n",
    "    sim_n = (v_a * v_n).sum(dim = 1)\n",
    "    sim_p = (v_a * v_p).sum(dim = 1)\n",
    "    return (sim_p > sim_n).type(torch.FloatTensor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5151)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(compute_loss(_dummy_anchors, _dummy_positives, _dummy_negatives))\n",
    "print(compute_recall(_dummy_anchors, _dummy_positives, _dummy_negatives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "max_len = 100\n",
    "batch_size = 32\n",
    "batches_per_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tnrange\n",
    "def iterate_minibatches(data, batch_size=32, max_len=None,\n",
    "                        max_batches=None, shuffle=True, verbose=True):\n",
    "    indices = np.arange(len(data))\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(indices)\n",
    "    if max_batches is not None:\n",
    "        indices = indices[: batch_size * max_batches]\n",
    "        \n",
    "    irange = tnrange if verbose else range\n",
    "    \n",
    "    for start in irange(0, len(indices), batch_size):\n",
    "        yield generate_batch([data[i] for i in indices[start : start + batch_size]], max_len=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For a difference, we'll ask __you__ to implement training loop this time.\n",
    "\n",
    "Here's a sketch of one epoch:\n",
    "1. iterate over __`batches_per_epoch`__ batches from __`train_data`__\n",
    "    * Compute loss, backprop, optimize\n",
    "    * Compute and accumulate recall\n",
    "    \n",
    "2. iterate over __`batches_per_epoch`__ batches from __`val_data`__\n",
    "    * Compute and accumulate recall\n",
    "    \n",
    "3. print stuff :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527692747de64b3db4ec39187d8770cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.82672\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44970832e234500b522530ff4e69774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.69097\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0858014f558046299134160b6203fb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.63133\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef9745b2fe14af8be0285685d05d9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.58873\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee9ec24601a40349d2fe041dc5c691b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.55738\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac2db5eb0e240fcbd89dece0f20fb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.53853\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79e72ab41cb471abd885853ef1d06b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.50307\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe34bc91e8943c0ae59d7d8cb4d9c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.50015\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697ba30847f643899ec5a4cdf0a96e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.47774\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df1acd9010b4fec9317852c051ac6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.46864\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e94142c46b4fad8d9effd293071551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.44636\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452d80f544d64f909866288cdb6c1c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.43096\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff832ac49564f90ba0c6f76fe44dd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.44157\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c6bc32f5904e14b7ab43b4013f7935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.41642\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef7955f937a4d9bac9a9058c1236f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.40748\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8ea5c6aa6141409910027cebcdbf5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.39624\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7562a9879e3410c98f52b099d72765c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.38050\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99cf9bf15ea44f5799a9156fbf8a7e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.38799\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c49aa0d5e624a968e08949737370eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.38765\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a813f29a7d4b06bc62436e0f040d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.38778\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f930829609424e6989bef09954ae0487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.35967\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bd1eb8a98143dfb70daf805c9ca9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.35906\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c36c0f3a3c4973a1d12624063a2f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.36295\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df97a40fc5f49f09af628244bf48d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.35497\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f6e719ee684d2190306f5649e39f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.34423\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e402e9914974434a88619a4f2ed1a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.33270\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9e103a4bd24ea481d8b21d53508610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.32945\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65792a42586414fa2778dac47aa7f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.33869\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c88f046e6f434e8664bee117da4d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-2a73a633e292>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torchEnv\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_i in range(num_epochs):\n",
    "    \n",
    "    print(\"Training:\")\n",
    "    train_loss = train_mae = train_batches = 0    \n",
    "    #model.train(True)\n",
    "    \n",
    "    for batch in iterate_minibatches(train_data, max_batches=512):\n",
    "        loss = compute_loss(*batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "    \n",
    "    print(\"\\tLoss:\\t%.5f\" % (train_loss / train_batches))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch_i in range(num_epochs):\n",
    "    \n",
    "    print(\"Training:\")\n",
    "    train_loss = train_batches = 0    \n",
    "    model.train(True)\n",
    "    \n",
    "    for batch in iterate_minibatches(data_train, max_batches=batches_per_epoch):\n",
    "        loss = compute_loss(*batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "    \n",
    "    print(\"\\tLoss:\\t%.5f\" % (train_loss / train_batches))\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    print(\"Validation:\")\n",
    "    val_loss = val_batches = 0\n",
    "    model.train(False)\n",
    "    \n",
    "    for batch in iterate_minibatches(val_data, shuffle=False):\n",
    "        loss = compute_loss(*batch)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        val_batches += 1\n",
    "        \n",
    "    print(\"\\tLoss:\\t%.5f\" % (val_loss / val_batches))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was William Scott Wilson's occupation?\n"
     ]
    }
   ],
   "source": [
    "Q = train_data[0][0]\n",
    "print(f'Question: {Q}')\n",
    "V_Q = question_vectorizer(Variable(torch.LongTensor(as_matrix([Q]))))\n",
    "answers = [train_data[i][1] for i in range(100)]\n",
    "V_A = answer_vectorizer(Variable(torch.LongTensor(as_matrix(answers))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.56246567\n",
      "Zen meditation became an important teaching due to it offering a process to calm one's mind.\n",
      "-0.9209545\n",
      "Japan mustered a mere 10,000 samurai to meet this threat.\n",
      "-0.95281345\n",
      "Samurai were many of the early exchange students, not directly because they were samurai, but because many samurai were literate and well-educated scholars.\n",
      "-1.0128446\n",
      "Bushido was formalized by several influential leaders and families before the Edo Period.\n",
      "-1.0128446\n",
      "Bushido was formalized by several influential leaders and families before the Edo Period.\n"
     ]
    }
   ],
   "source": [
    "sim = (V_A * V_Q).sum(1).data.numpy()\n",
    "for i in np.argsort(-sim)[:5]:\n",
    "    print(sim[i])\n",
    "    print(answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "69ee5b52104d471ca7bfb32ba4309743": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "7b18f460e231498eaafa7653026e98e0": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

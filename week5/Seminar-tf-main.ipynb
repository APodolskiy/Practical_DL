{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salary prediction via Deep NLP methods\n",
    "A recent Kaggle competition (merely 5 years) propose you to find out who is the most well-paid professional\n",
    "\n",
    "We are gonna solve regression task.\n",
    "\n",
    "Competition is available here: https://www.kaggle.com/c/job-salary-prediction\n",
    "\n",
    "![Hobby](https://imgs.xkcd.com/comics/extrapolating.png)\n",
    "\n",
    "In this notebook you will learn\n",
    " - Data preprocessing for NLP or the most annoying part of data scientist's job\n",
    " - Convolutional Neural Networks for texts\n",
    " - Constructing you NN with scripting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Dataset is consists of job data. Most of it is an unstructured text. You should predict annual salary\n",
    "\n",
    "Download here: https://yadi.sk/d/vVEOWPFY3NruT7 or from competition\n",
    "\n",
    "You need to download Train_rev1.zip and unpack it\n",
    "## Main fields\n",
    "\n",
    "Title - A freetext field supplied to us by the job advertiser as the Title of the job ad.  Normally this is a summary of the job title or role.\n",
    "\n",
    "FullDescription - The full text of the job ad as provided by the job advertiser.  Where you see ***s, we have stripped values from the description in order to ensure that no salary information appears within the descriptions.  There may be some collateral damage here where we have also removed other numerics.\n",
    "\n",
    "LocationRaw - The freetext location as provided by the job advertiser.\n",
    "\n",
    "LocationNormalized - Adzuna's normalised location from within our own location tree, interpreted by us based on the raw location.  Our normaliser is not perfect!\n",
    "\n",
    "ContractType - full_time or part_time, interpreted by Adzuna from description or a specific additional field we received from the advertiser.\n",
    "\n",
    "ContractTime - permanent or contract, interpreted by Adzuna from description or a specific additional field we received from the advertiser.\n",
    "\n",
    "Company - the name of the employer as supplied to us by the job advertiser.\n",
    "\n",
    "Category - which of 30 standard job categories this ad fits into, inferred in a very messy way based on the source the ad came from.  We know there is a lot of noise and error in this field.\n",
    "\n",
    "## Desiered field\n",
    "SalaryRaw - the freetext salary field we received in the job advert from the advertiser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12612628</td>\n",
       "      <td>Engineering Systems Analyst</td>\n",
       "      <td>Engineering Systems Analyst Dorking Surrey Sal...</td>\n",
       "      <td>Dorking, Surrey, Surrey</td>\n",
       "      <td>Dorking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12612830</td>\n",
       "      <td>Stress Engineer Glasgow</td>\n",
       "      <td>Stress Engineer Glasgow Salary **** to **** We...</td>\n",
       "      <td>Glasgow, Scotland, Scotland</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 35000/annum 25-35K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12612844</td>\n",
       "      <td>Modelling and simulation analyst</td>\n",
       "      <td>Mathematical Modeller / Simulation Analyst / O...</td>\n",
       "      <td>Hampshire, South East, South East</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 40000/annum 20-40K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12613049</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 30000/annum 25K-30K negotiable</td>\n",
       "      <td>27500</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12613647</td>\n",
       "      <td>Pioneer, Miser Engineering Systems Analyst</td>\n",
       "      <td>Pioneer, Miser  Engineering Systems Analyst Do...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title  \\\n",
       "0  12612628                        Engineering Systems Analyst   \n",
       "1  12612830                            Stress Engineer Glasgow   \n",
       "2  12612844                   Modelling and simulation analyst   \n",
       "3  12613049  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  12613647         Pioneer, Miser Engineering Systems Analyst   \n",
       "\n",
       "                                     FullDescription  \\\n",
       "0  Engineering Systems Analyst Dorking Surrey Sal...   \n",
       "1  Stress Engineer Glasgow Salary **** to **** We...   \n",
       "2  Mathematical Modeller / Simulation Analyst / O...   \n",
       "3  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  Pioneer, Miser  Engineering Systems Analyst Do...   \n",
       "\n",
       "                         LocationRaw LocationNormalized ContractType  \\\n",
       "0            Dorking, Surrey, Surrey            Dorking          NaN   \n",
       "1        Glasgow, Scotland, Scotland            Glasgow          NaN   \n",
       "2  Hampshire, South East, South East          Hampshire          NaN   \n",
       "3     Surrey, South East, South East             Surrey          NaN   \n",
       "4     Surrey, South East, South East             Surrey          NaN   \n",
       "\n",
       "  ContractTime                       Company          Category  \\\n",
       "0    permanent  Gregory Martin International  Engineering Jobs   \n",
       "1    permanent  Gregory Martin International  Engineering Jobs   \n",
       "2    permanent  Gregory Martin International  Engineering Jobs   \n",
       "3    permanent  Gregory Martin International  Engineering Jobs   \n",
       "4    permanent  Gregory Martin International  Engineering Jobs   \n",
       "\n",
       "                                SalaryRaw  SalaryNormalized        SourceName  \n",
       "0              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  \n",
       "1              25000 - 35000/annum 25-35K             30000  cv-library.co.uk  \n",
       "2              20000 - 40000/annum 20-40K             30000  cv-library.co.uk  \n",
       "3  25000 - 30000/annum 25K-30K negotiable             27500  cv-library.co.uk  \n",
       "4              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame.from_csv('Train_rev1.csv', index_col=None)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.ContractType = X.ContractType.astype(str)\n",
    "X.ContractTime = X.ContractTime.astype(str)\n",
    "X.Company = X.Company.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = X['SalaryNormalized'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del X['Id'],  X['SalaryRaw'], X['SourceName'], X['SalaryNormalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9763080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(Y, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "At this very stage we will distill valuable data out of the dataset\n",
    "\n",
    "First of all - let's remove rare tokens and finalaize our dictionary\n",
    "\n",
    "We count all tokens ever occurred in any of `text_columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'FullDescription', 'LocationRaw', 'LocationNormalized', 'ContractType', 'ContractTime', 'Company', 'Category']\n"
     ]
    }
   ],
   "source": [
    "text_columns = list(X.columns)\n",
    "print(text_columns)\n",
    "categorial_colums = ['ContractType',\n",
    " 'ContractTime',\n",
    " 'Company',\n",
    " 'Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code is below. \n",
    "\n",
    "Remember to apply .lower() to all strings before tokenization\n",
    "\n",
    "Consider using tqdm_notebook for not dying  during the iteration process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+|\\d+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenize(value):\n",
    "    \"\"\" Returns a list of string tokens\n",
    "    Arguments:\n",
    "    value -- string to tokenize\n",
    "    \"\"\"\n",
    "    return tokenizer.tokenize(value.lower())\n",
    "\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert token_counts.most_common(1)[0][1] in  range(2600000, 2700000)\n",
    "assert len(token_counts) in range(200000, 210000)\n",
    "print('Correct!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to actually build token dict. We will use only words that occur more than `min_count` times in dataset\n",
    "\n",
    "Fill two dict mappings id->token and token->id\n",
    "\n",
    "**Minimum id is 2**, because 0 is reserved for padding and 1 is UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "\n",
    "tokens = <tokens from token_counts keys that had at least min_count occurrences throughout the dataset>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert type(tokens)==list\n",
    "assert len(tokens) in range(32000,34000)\n",
    "assert 'me' in tokens\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_size = len(tokens)+2\n",
    "id_to_word = dict()\n",
    "word_to_id = dict()\n",
    "\n",
    "token_to_id = <your code here>\n",
    "\n",
    "id_to_token = <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert token_to_id['me'] != token_to_id['woods']\n",
    "assert token_to_id[id_to_token[42]]==42\n",
    "assert len(token_to_id)==len(tokens)\n",
    "assert 0 not in id_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, UNK):\n",
    "    '''This function gets a string array and transforms it to padded token matrix\n",
    "    Remember to:\n",
    "     - Transform a string to list of tokens\n",
    "     - Transform each token to it ids (if not in the dict, replace with UNK)\n",
    "     - Pad each line to max_len'''\n",
    "    max_len = 0\n",
    "    token_matrix = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    <YOUR CODE HERE>\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = vectorize([\"Hello, adshkjasdhkas\", \"data\"], token_to_id, 1)\n",
    "assert test.shape==(2,2)\n",
    "assert (test[:,1]==(1,0)).all()\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you successfully completed all tasks by this moment, you get (2 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True deep learning\n",
    "\n",
    "Now we will define our convolutional neural network.\n",
    "\n",
    "We will think about categorical fields as a sequential - but we won't apply CNN to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialaize some placeholders for data\n",
    "\n",
    "What size it should  be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "placeholders = dict()\n",
    "for col in text_columns:\n",
    "    placeholders[col] = tf.placeholder(dtype=tf.int32,\n",
    "                                        shape=[None, None],\n",
    "                                        name=\"word_ids_%s\"%col)\n",
    "    \n",
    "true_y = tf.placeholder(dtype=tf.float32, shape = [None], name=\"true_y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are vector representations for tokens. Basically it is just a table where each token (represented by it's id) has a vector representing its sense.\n",
    "\n",
    "It will be learned simultaneously with other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_size = 50\n",
    "word_embeddings_matrix = tf.get_variable(name=\"word_embeddings_matrix\",\n",
    "                                         dtype=tf.float32,\n",
    "                                         initializer=tf.random_normal(\n",
    "                                             shape=[dict_size, embeddings_size], #<your_code_here>\n",
    "                                             stddev=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep nets\n",
    "Below we are gonna define some network architectures corresponding to each input (a column from a source data)\n",
    "\n",
    "`tf.layers.conv1d()` has three parameters. \n",
    " - inputs is a tensor with [batch_size, time, embedding_dimension]\n",
    " - filters is an int number which means number of channels on the output\n",
    " - kernel_size is an int number which means a size of input data which will be convoluted\n",
    "\n",
    "**Please, specify a padding='same'. This will prevent errors in case of low **\n",
    "`tf.layers.conv1d()` returns a tensor with dimension  [batch_size, time, filters]\n",
    "\n",
    "`tf.layers.max_pooling1d()` is prerry much the same. The only thing you need to specify - pool size and strides.\n",
    "\n",
    "For global pooling you can use `tf.reduce_max()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.layers.max_pooling1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dream_neural_net(word_ids, name):\n",
    "    \"\"\"Function takes an int32 matrix [batch_size, time] and returnes infered [batch_size, dense_vector] \n",
    "    matrix with a conv and pooling layers, finished by a global pooling\"\"\"\n",
    "    with tf.variable_scope(name_or_scope=\"conv_\" + name):\n",
    "        output = tf.nn.embedding_lookup(params=word_embeddings_matrix,\n",
    "                                             ids=word_ids) #returns [batch_size, time, embedding_dimension]\n",
    "        <YOUR CODE HERE>\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dream_neural_net_categorial(word_ids, name):\n",
    "    \"\"\"Function takes an int32 matrix [batch_size, time] and returnes infered [batch_size, latent_dim] \n",
    "    matrix without a conv and pooling layers, only dense layers\"\"\"\n",
    "    word_embeddings = tf.nn.embedding_lookup(params=word_embeddings_matrix,\n",
    "                                         ids=word_ids) #returns [batch_size, time, embedding_dimension]\n",
    "\n",
    "    <YOUR CODE HERE>\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to combine all architectures. In a dict below you can match input type and an architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nets_types = {'FullDescription': dream_neural_net, \n",
    " 'LocationRaw': dream_neural_net, \n",
    " 'Title': dream_neural_net, \n",
    "              \n",
    " 'LocationNormalized': dream_neural_net_categorial,             \n",
    " 'ContractType': dream_neural_net_categorial, \n",
    " 'ContractTime': dream_neural_net_categorial, \n",
    " 'Company': dream_neural_net_categorial, \n",
    " 'Category':dream_neural_net_categorial\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here we apply specified architecture for each input \n",
    "outputs_to_concat = [nets_types[name](word_ids, name) for name, word_ids in placeholders.items()] \n",
    "dense_repr = tf.concat(outputs_to_concat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(dense_repr.get_shape().as_list())==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_to_number(dense_inputs):\n",
    "    \"\"\"\n",
    "    Function reduces a concated matrix [batch_size, latent_dim] and returns a net output \n",
    "    which will be directly used for inference\n",
    "    \"\"\"\n",
    "    <YOUR CODE HERE>\n",
    "    output = tf.contrib.layers.fully_connected(inputs=output,\n",
    "                                                   num_outputs=<YOUR CODE HERE>,\n",
    "                                                   activation_fn=None,\n",
    "                                                   weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                   biases_initializer=tf.zeros_initializer(),\n",
    "                                                   scope=\"dense_2\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net_output = reduce_to_number(dense_repr)\n",
    "predicted_y = tf.reshape(net_output, (-1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert net_output.get_shape().as_list()==[None, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Remember - we have a regression task, what would be the loss?\n",
    "\n",
    "Also we will estimate a target metric for a competition - **Mean absolute error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = <YOUR CODE HERE>\n",
    "mean_abs_error = tf.reduce_mean(tf.abs(predicted_y - true_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = <YOUR CODE HERE>\n",
    "global_step = tf.Variable(initial_value=0)\n",
    "train_op = optimizer.minimize(\n",
    "  loss=loss,\n",
    "  global_step=global_step, var_list=tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process\n",
    "\n",
    "The last thing before we can run the whole monster - define train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_batches(X, Y=None):\n",
    "    \"\"\"Takes a part of pandas DF\n",
    "    Returns a pair (X_batch, Y_batch) or only X_batch, where \n",
    "     - X_batch is a dict {key: value} - where a key is name of column and a value is \n",
    "    a matrix which will be passed to corresponding input.\n",
    "     - Y_batch is just a vector with output values\n",
    "    \"\"\"\n",
    "    size = len(X)\n",
    "    i = 0\n",
    "    while i < len(X):\n",
    "        <YOUR CODE HERE>\n",
    "        \n",
    "        if Y is None:\n",
    "            yield X_batch \n",
    "        else:\n",
    "            yield X_batch, Y_batch\n",
    "        i+=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input(X_batch,Y_batch=None):\n",
    "    \"\"\"Function takes a batch from iterate_batches and returns a feed_dictionary with placeholders\"\"\"\n",
    "    feed_dct = {placeholders[k]: X_batch[k] for k in text_columns} \n",
    "    if Y_batch is not None:\n",
    "        feed_dct[true_y] = Y_batch\n",
    "    return feed_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    \"\"\"Here you go over X_val and Y_val and count mean errors.\n",
    "    Use iterate_batches and get_input\"\"\"\n",
    "    MSE, AE = 0, 0\n",
    "    batches = 0\n",
    "    <YOUR CODE HERE>\n",
    "    MSE/=batches\n",
    "    AE/=batches\n",
    "    return (MSE, AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    \"Function inferes prediction by dict of inputs\"\n",
    "    return sess.run(predicted_y, get_input(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for first_batch in iterate_batches(X_train, Y_train):\n",
    "    break\n",
    "assert len(first_batch) == 2\n",
    "assert type(first_batch[0]) == dict\n",
    "assert first_batch[1].shape[0]==batch_size\n",
    "assert np.unique([inp.shape[0] for inp in first_batch[0].values()])==batch_size\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MSE, AE = validate()\n",
    "assert MSE < 1e10\n",
    "assert AE < 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_batches = len(X_train)/batch_size\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Finally it is time to run training\n",
    "\n",
    "First, shuffle the data\n",
    "\n",
    "\n",
    "By the way, if the trainig processes starts and you achive at least **20k** AE error on validation, you get additional **(3 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(X_train))\n",
    "X_train = X_train.iloc[p]\n",
    "Y_train = Y_train.iloc[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(num_epochs):\n",
    "    for i, (X_batch, Y_batch) in  enumerate(iterate_batches(X_train, Y_train)):\n",
    "        _, step, current_loss, abs_error =  sess.run([train_op, global_step, loss, mean_abs_error], get_input(X_batch,Y_batch))\n",
    "        print \"Current step: %s. Current loss is %s. Absolute error is %s\" % (step, current_loss, abs_error)\n",
    "        if i%30==0:\n",
    "            print(\"Validation. MSE: %s, AE: %s\" %validate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MSE, AE = validate()\n",
    "assert AE < 20000\n",
    "print (\"I earned 3 pts with %s absolute error!\" % AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further experiments and home assignment\n",
    " - **(1 pt)** Add visualisation of train and val loss. Try to use some smoothing to make plots more readable \n",
    " - **(2 pt)** Try different CNN architectures. Vary kernel size, number of filters. Find out if there is some change to a training process.\n",
    " - Try to use different architectures for different inputs. Maybe a smaller architecture would be fine for description field and more complex for a title.\n",
    " - Find out the best **embedding size** value \n",
    " - Add dropout, and some regularisation \n",
    " \n",
    " **(2++ pts) for all experimenting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See in the next series\n",
    " - Recurrent neural networks\n",
    "  - Why everybody like them\n",
    "  - Why everybody hate them\n",
    " - Attention for text processing\n",
    "  - How to boost your NLP model performance by implementing recent DL paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python TF",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

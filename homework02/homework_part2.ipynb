{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework 2.2: The Quest For A Better Network\n",
    "\n",
    "In this assignment you will build a monster network to solve CIFAR10 image classification.\n",
    "\n",
    "This notebook is intended as a sequel to seminar 3, please give it a try if you haven't done so yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(please read it at least diagonally)\n",
    "\n",
    "* The ultimate quest is to create a network that has as high __accuracy__ as you can push it.\n",
    "* There is a __mini-report__ at the end that you will have to fill in. We recommend reading it first and filling it while you iterate.\n",
    " \n",
    "## Grading\n",
    "* starting at zero points\n",
    "* +20% for describing your iteration path in a report below.\n",
    "* +20% for building a network that gets above 20% accuracy\n",
    "* +10% for beating each of these milestones on __TEST__ dataset:\n",
    "    * 50% (50% points)\n",
    "    * 60% (60% points)\n",
    "    * 65% (70% points)\n",
    "    * 70% (80% points)\n",
    "    * 75% (90% points)\n",
    "    * 80% (full points)\n",
    "    \n",
    "## Restrictions\n",
    "* Please do NOT use pre-trained networks for this assignment until you reach 80%.\n",
    " * In other words, base milestones must be beaten without pre-trained nets (and such net must be present in the e-mail). After that, you can use whatever you want.\n",
    "* you __can__ use validation data for training, but you __can't'__ do anything with test data apart from running the evaluation procedure.\n",
    "\n",
    "## Tips on what can be done:\n",
    "\n",
    "\n",
    " * __Network size__\n",
    "   * MOAR neurons, \n",
    "   * MOAR layers, ([torch.nn docs](http://pytorch.org/docs/master/nn.html))\n",
    "\n",
    "   * Nonlinearities in the hidden layers\n",
    "     * tanh, relu, leaky relu, etc\n",
    "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
    "\n",
    "   * Ph'nglui mglw'nafh Cthulhu R'lyeh wgah'nagl fhtagn!\n",
    "\n",
    "\n",
    "### The main rule of prototyping: one change at a time\n",
    "   * By now you probably have several ideas on what to change. By all means, try them out! But there's a catch: __never test several new things at once__.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
    "   * Some networks converge over 5 epochs, others - over 500.\n",
    "   * Way to go: stop when validation score is 10 iterations past maximum\n",
    "   * You should certainly use adaptive optimizers\n",
    "     * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
    "     * Converge faster and sometimes reach better optima\n",
    "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
    "   * __BatchNormalization__ (nn.BatchNorm2d) for the win!\n",
    "     * Sometimes more batch normalization is better.\n",
    "   * __Regularize__ to prevent overfitting\n",
    "     * Add some L2 weight norm to the loss function, PyTorch will do the rest\n",
    "       * Can be done manually or like [this](https://discuss.pytorch.org/t/simple-l2-regularization/139/2).\n",
    "     * Dropout (`nn.Dropout`) - to prevent overfitting\n",
    "       * Don't overdo it. Check if it actually makes your network better\n",
    "   \n",
    "### Convolution architectures\n",
    "   * This task __can__ be solved by a sequence of convolutions and poolings with batch_norm and ReLU seasoning, but you shouldn't necessarily stop there.\n",
    "   * [Inception family](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/), [ResNet family](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035?gi=9018057983ca), [Densely-connected convolutions (exotic)](https://arxiv.org/abs/1608.06993), [Capsule networks (exotic)](https://arxiv.org/abs/1710.09829)\n",
    "   * Please do try a few simple architectures before you go for resnet-152.\n",
    "   * Warning! Training convolutional networks can take long without GPU. That's okay.\n",
    "     * If you are CPU-only, we still recomment that you try a simple convolutional architecture\n",
    "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
    "     * Make reasonable layer size estimates. A 128-neuron first convolution is likely an overkill.\n",
    "     * __To reduce computation__ time by a factor in exchange for some accuracy drop, try using __stride__ parameter. A stride=2 convolution should take roughly 1/4 of the default (stride=1) one.\n",
    " \n",
    "   \n",
    "### Data augmemntation\n",
    "   * getting 5x as large dataset for free is a great \n",
    "     * Zoom-in+slice = move\n",
    "     * Rotate+zoom(to remove black stripes)\n",
    "     * Add Noize (gaussian or bernoulli)\n",
    "   * Simple way to do that (if you have PIL/Image): \n",
    "     * ```from scipy.misc import imrotate,imresize```\n",
    "     * and a few slicing\n",
    "     * Other cool libraries: cv2, skimake, PIL/Pillow\n",
    "   * A more advanced way is to use torchvision transforms:\n",
    "    ```\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10(root=path_to_cifar_like_in_seminar, train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "    ```\n",
    "   * Or use this tool from Keras (requires theano/tensorflow): [tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), [docs](https://keras.io/preprocessing/image/)\n",
    "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them.\n",
    "   \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "   \n",
    "There is a template for your solution below that you can opt to use or throw away and write it your way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 3, 32, 32) (40000,)\n"
     ]
    }
   ],
   "source": [
    "from cifar import load_cifar10\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_cifar10(\"cifar_data\")\n",
    "class_names = np.array(['airplane','automobile ','bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck'])\n",
    "\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # Second conv block\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # Third conv block\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # Dense block\n",
    "            Flatten(),\n",
    "            nn.Linear(in_features=4*4*128, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # Second conv block\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # Third conv block\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # Dense block\n",
    "            Flatten(),\n",
    "            nn.Linear(in_features=4*4*128, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X_batch, y_batch, device):\n",
    "    X_batch = torch.FloatTensor(X_batch).to(device)\n",
    "    y_batch = torch.LongTensor(y_batch).to(device)\n",
    "    logits = model(X_batch)\n",
    "    return F.cross_entropy(logits, y_batch).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Training __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() and use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(X, y, batchsize):\n",
    "    indices = np.random.permutation(np.arange(len(X)))\n",
    "    for start in range(0, len(indices), batchsize):\n",
    "        ix = indices[start: start + batchsize]\n",
    "        yield X[ix], y[ix]\n",
    "        \n",
    "model = Model().to(device)        \n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 50 took 92.032s\n",
      "  training loss (in-iteration): \t1.583494\n",
      "  validation accuracy: \t\t\t55.42 %\n",
      "Epoch 2 of 50 took 92.241s\n",
      "  training loss (in-iteration): \t1.168650\n",
      "  validation accuracy: \t\t\t65.43 %\n",
      "Epoch 3 of 50 took 92.468s\n",
      "  training loss (in-iteration): \t0.956582\n",
      "  validation accuracy: \t\t\t71.81 %\n",
      "Epoch 4 of 50 took 92.205s\n",
      "  training loss (in-iteration): \t0.811622\n",
      "  validation accuracy: \t\t\t72.00 %\n",
      "Epoch 5 of 50 took 91.657s\n",
      "  training loss (in-iteration): \t0.712388\n",
      "  validation accuracy: \t\t\t77.25 %\n",
      "Epoch 6 of 50 took 93.792s\n",
      "  training loss (in-iteration): \t0.626193\n",
      "  validation accuracy: \t\t\t75.79 %\n",
      "Epoch 7 of 50 took 93.265s\n",
      "  training loss (in-iteration): \t0.558329\n",
      "  validation accuracy: \t\t\t76.59 %\n",
      "Epoch 8 of 50 took 92.900s\n",
      "  training loss (in-iteration): \t0.496544\n",
      "  validation accuracy: \t\t\t79.93 %\n",
      "Epoch 9 of 50 took 92.410s\n",
      "  training loss (in-iteration): \t0.439909\n",
      "  validation accuracy: \t\t\t80.68 %\n",
      "Epoch 10 of 50 took 91.485s\n",
      "  training loss (in-iteration): \t0.388185\n",
      "  validation accuracy: \t\t\t81.89 %\n",
      "Epoch 11 of 50 took 91.272s\n",
      "  training loss (in-iteration): \t0.358791\n",
      "  validation accuracy: \t\t\t81.59 %\n",
      "Epoch 12 of 50 took 91.682s\n",
      "  training loss (in-iteration): \t0.313847\n",
      "  validation accuracy: \t\t\t81.55 %\n",
      "Epoch 13 of 50 took 92.539s\n",
      "  training loss (in-iteration): \t0.290082\n",
      "  validation accuracy: \t\t\t81.27 %\n",
      "Epoch 14 of 50 took 92.632s\n",
      "  training loss (in-iteration): \t0.258370\n",
      "  validation accuracy: \t\t\t82.10 %\n",
      "Epoch 15 of 50 took 94.123s\n",
      "  training loss (in-iteration): \t0.237751\n",
      "  validation accuracy: \t\t\t81.45 %\n",
      "Epoch 16 of 50 took 94.393s\n",
      "  training loss (in-iteration): \t0.224044\n",
      "  validation accuracy: \t\t\t80.84 %\n",
      "Epoch 17 of 50 took 91.570s\n",
      "  training loss (in-iteration): \t0.192744\n",
      "  validation accuracy: \t\t\t82.64 %\n",
      "Epoch 18 of 50 took 90.132s\n",
      "  training loss (in-iteration): \t0.193745\n",
      "  validation accuracy: \t\t\t81.95 %\n",
      "Epoch 19 of 50 took 91.957s\n",
      "  training loss (in-iteration): \t0.173526\n",
      "  validation accuracy: \t\t\t82.09 %\n",
      "Epoch 20 of 50 took 94.081s\n",
      "  training loss (in-iteration): \t0.164436\n",
      "  validation accuracy: \t\t\t81.38 %\n",
      "Epoch 21 of 50 took 92.063s\n",
      "  training loss (in-iteration): \t0.154968\n",
      "  validation accuracy: \t\t\t82.70 %\n",
      "Epoch 22 of 50 took 91.647s\n",
      "  training loss (in-iteration): \t0.148477\n",
      "  validation accuracy: \t\t\t81.75 %\n",
      "Epoch 23 of 50 took 90.160s\n",
      "  training loss (in-iteration): \t0.146810\n",
      "  validation accuracy: \t\t\t81.97 %\n",
      "Epoch 24 of 50 took 90.594s\n",
      "  training loss (in-iteration): \t0.130885\n",
      "  validation accuracy: \t\t\t82.04 %\n",
      "Epoch 25 of 50 took 91.766s\n",
      "  training loss (in-iteration): \t0.134584\n",
      "  validation accuracy: \t\t\t81.81 %\n",
      "Epoch 26 of 50 took 91.813s\n",
      "  training loss (in-iteration): \t0.127146\n",
      "  validation accuracy: \t\t\t80.72 %\n",
      "Epoch 27 of 50 took 92.629s\n",
      "  training loss (in-iteration): \t0.117433\n",
      "  validation accuracy: \t\t\t82.13 %\n",
      "Epoch 28 of 50 took 92.809s\n",
      "  training loss (in-iteration): \t0.114794\n",
      "  validation accuracy: \t\t\t81.69 %\n",
      "Epoch 29 of 50 took 90.815s\n",
      "  training loss (in-iteration): \t0.105965\n",
      "  validation accuracy: \t\t\t81.93 %\n",
      "Epoch 30 of 50 took 90.051s\n",
      "  training loss (in-iteration): \t0.110518\n",
      "  validation accuracy: \t\t\t81.55 %\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 50 # total amount of full passes over training data\n",
    "batch_size = 32  # number of samples processed in one SGD iteration\n",
    "\n",
    "early_stop = 0\n",
    "validation_acc_max = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    start_time = time.time()\n",
    "    model.train(True) # enable dropout / batch_norm training behavior\n",
    "    for X_batch, y_batch in iterate_minibatches(X_train, y_train, batch_size):\n",
    "        # train on batch\n",
    "        loss = compute_loss(X_batch, y_batch, device)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    # And a full pass over the validation data:\n",
    "    with torch.no_grad():\n",
    "        model.train(False) # disable dropout / use averages for batch_norm\n",
    "        for X_batch, y_batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "            logits = model(torch.FloatTensor(X_batch).to(device))\n",
    "            y_pred = logits.max(1)[1].data.cpu().numpy()\n",
    "            val_accuracy.append(np.mean(y_batch == y_pred))\n",
    "    \n",
    "    validation_accuracy = np.mean(val_accuracy[-len(X_val) // batch_size :])\n",
    "    \n",
    "    if validation_accuracy > validation_acc_max:\n",
    "        validation_acc_max = validation_accuracy\n",
    "        early_stop = 0\n",
    "    else:\n",
    "        early_stop += 1\n",
    "        if early_stop >= 10:\n",
    "            break\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-len(X_train) // batch_size :])))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(validation_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t81.04 %\n",
      "Achievement unlocked: 110lvl Warlock!\n"
     ]
    }
   ],
   "source": [
    "# ELU last\n",
    "model.train(False) # disable dropout / use averages for batch_norm\n",
    "test_batch_acc = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in iterate_minibatches(X_test, y_test, 500):\n",
    "        logits = model(torch.FloatTensor(X_batch).to(device))\n",
    "        y_pred = logits.max(1)[1].data.cpu().numpy()\n",
    "        test_batch_acc.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "test_accuracy = np.mean(test_batch_acc)\n",
    "    \n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_accuracy * 100))\n",
    "\n",
    "if test_accuracy * 100 > 95:\n",
    "    print(\"Double-check, than consider applying for NIPS'17. SRSly.\")\n",
    "elif test_accuracy * 100 > 90:\n",
    "    print(\"U'r freakin' amazin'!\")\n",
    "elif test_accuracy * 100 > 80:\n",
    "    print(\"Achievement unlocked: 110lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 70:\n",
    "    print(\"Achievement unlocked: 80lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 60:\n",
    "    print(\"Achievement unlocked: 70lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 50:\n",
    "    print(\"Achievement unlocked: 60lvl Warlock!\")\n",
    "else:\n",
    "    print(\"We need more magic! Follow instructons below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t81.12 %\n",
      "Achievement unlocked: 110lvl Warlock!\n"
     ]
    }
   ],
   "source": [
    "# RelU best\n",
    "model.train(False) # disable dropout / use averages for batch_norm\n",
    "test_batch_acc = []\n",
    "for X_batch, y_batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    logits = model(torch.FloatTensor(X_batch))\n",
    "    y_pred = logits.max(1)[1].data.numpy()\n",
    "    test_batch_acc.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "test_accuracy = np.mean(test_batch_acc)\n",
    "    \n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_accuracy * 100))\n",
    "\n",
    "if test_accuracy * 100 > 95:\n",
    "    print(\"Double-check, than consider applying for NIPS'17. SRSly.\")\n",
    "elif test_accuracy * 100 > 90:\n",
    "    print(\"U'r freakin' amazin'!\")\n",
    "elif test_accuracy * 100 > 80:\n",
    "    print(\"Achievement unlocked: 110lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 70:\n",
    "    print(\"Achievement unlocked: 80lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 60:\n",
    "    print(\"Achievement unlocked: 70lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 50:\n",
    "    print(\"Achievement unlocked: 60lvl Warlock!\")\n",
    "else:\n",
    "    print(\"We need more magic! Follow instructons below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation part\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device)        \n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.array((0.4914, 0.4822, 0.4465))\n",
    "stds = np.array((0.2023, 0.1994, 0.2010))\n",
    "\n",
    "transform_augment = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomRotation([-30, 30]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CIFAR10(\"./cifar_data/\", train=True, transform=transform_augment)\n",
    "valid_loader = CIFAR10(\"./cifar_data/\", train=True, transform=transform_valid)\n",
    "\n",
    "valid_size = 0.2\n",
    "num_train = len(train_loader)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "np.random.shuffle(indices)\n",
    "batch_size = 32\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_loader, \n",
    "                                              batch_size=batch_size,\n",
    "                                              num_workers=1,\n",
    "                                              sampler=train_sampler)\n",
    "\n",
    "\n",
    "valid_batch_gen = torch.utils.data.DataLoader(valid_loader,\n",
    "                                              batch_size=batch_size,\n",
    "                                              num_workers=1,\n",
    "                                              sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "test_loader = CIFAR10(\"./cifar_data/\", train=False, transform=transform_test)\n",
    "test_batch_gen = torch.utils.data.DataLoader(test_loader,\n",
    "                                             batch_size=50,\n",
    "                                             num_workers=1,\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 66.026s\n",
      "  training loss (in-iteration): \t1.495219\n",
      "  validation accuracy: \t\t\t51.50 %\n",
      "Epoch 2 of 100 took 73.387s\n",
      "  training loss (in-iteration): \t1.347932\n",
      "  validation accuracy: \t\t\t60.84 %\n",
      "Epoch 3 of 100 took 74.266s\n",
      "  training loss (in-iteration): \t1.251241\n",
      "  validation accuracy: \t\t\t62.37 %\n",
      "Epoch 4 of 100 took 75.992s\n",
      "  training loss (in-iteration): \t1.167524\n",
      "  validation accuracy: \t\t\t66.00 %\n",
      "Epoch 5 of 100 took 75.278s\n",
      "  training loss (in-iteration): \t1.107334\n",
      "  validation accuracy: \t\t\t68.35 %\n",
      "Epoch 6 of 100 took 73.836s\n",
      "  training loss (in-iteration): \t1.052632\n",
      "  validation accuracy: \t\t\t70.33 %\n",
      "Epoch 7 of 100 took 69.477s\n",
      "  training loss (in-iteration): \t1.011433\n",
      "  validation accuracy: \t\t\t73.03 %\n",
      "Epoch 8 of 100 took 71.379s\n",
      "  training loss (in-iteration): \t0.969486\n",
      "  validation accuracy: \t\t\t72.83 %\n",
      "Epoch 9 of 100 took 70.848s\n",
      "  training loss (in-iteration): \t0.933651\n",
      "  validation accuracy: \t\t\t74.10 %\n",
      "Epoch 10 of 100 took 71.537s\n",
      "  training loss (in-iteration): \t0.899280\n",
      "  validation accuracy: \t\t\t74.97 %\n",
      "Epoch 11 of 100 took 72.710s\n",
      "  training loss (in-iteration): \t0.876197\n",
      "  validation accuracy: \t\t\t76.67 %\n",
      "Epoch 12 of 100 took 71.223s\n",
      "  training loss (in-iteration): \t0.850600\n",
      "  validation accuracy: \t\t\t76.94 %\n",
      "Epoch 13 of 100 took 70.749s\n",
      "  training loss (in-iteration): \t0.819687\n",
      "  validation accuracy: \t\t\t77.39 %\n",
      "Epoch 14 of 100 took 69.865s\n",
      "  training loss (in-iteration): \t0.798300\n",
      "  validation accuracy: \t\t\t78.20 %\n",
      "Epoch 15 of 100 took 69.766s\n",
      "  training loss (in-iteration): \t0.788681\n",
      "  validation accuracy: \t\t\t78.60 %\n",
      "Epoch 16 of 100 took 68.243s\n",
      "  training loss (in-iteration): \t0.768146\n",
      "  validation accuracy: \t\t\t79.68 %\n",
      "Epoch 17 of 100 took 68.699s\n",
      "  training loss (in-iteration): \t0.741202\n",
      "  validation accuracy: \t\t\t80.32 %\n",
      "Epoch 18 of 100 took 67.109s\n",
      "  training loss (in-iteration): \t0.735777\n",
      "  validation accuracy: \t\t\t80.36 %\n",
      "Epoch 19 of 100 took 67.627s\n",
      "  training loss (in-iteration): \t0.721265\n",
      "  validation accuracy: \t\t\t80.64 %\n",
      "Epoch 20 of 100 took 67.165s\n",
      "  training loss (in-iteration): \t0.706391\n",
      "  validation accuracy: \t\t\t81.53 %\n",
      "Epoch 21 of 100 took 67.466s\n",
      "  training loss (in-iteration): \t0.696639\n",
      "  validation accuracy: \t\t\t80.60 %\n",
      "Epoch 22 of 100 took 67.215s\n",
      "  training loss (in-iteration): \t0.687125\n",
      "  validation accuracy: \t\t\t80.17 %\n",
      "Epoch 23 of 100 took 67.417s\n",
      "  training loss (in-iteration): \t0.678511\n",
      "  validation accuracy: \t\t\t82.65 %\n",
      "Epoch 24 of 100 took 66.801s\n",
      "  training loss (in-iteration): \t0.669520\n",
      "  validation accuracy: \t\t\t81.92 %\n",
      "Epoch 25 of 100 took 66.990s\n",
      "  training loss (in-iteration): \t0.666267\n",
      "  validation accuracy: \t\t\t82.09 %\n",
      "Epoch 26 of 100 took 67.404s\n",
      "  training loss (in-iteration): \t0.655496\n",
      "  validation accuracy: \t\t\t82.43 %\n",
      "Epoch 27 of 100 took 67.310s\n",
      "  training loss (in-iteration): \t0.643649\n",
      "  validation accuracy: \t\t\t82.02 %\n",
      "Epoch 28 of 100 took 67.284s\n",
      "  training loss (in-iteration): \t0.634330\n",
      "  validation accuracy: \t\t\t82.41 %\n",
      "Epoch 29 of 100 took 67.545s\n",
      "  training loss (in-iteration): \t0.632714\n",
      "  validation accuracy: \t\t\t82.76 %\n",
      "Epoch 30 of 100 took 67.492s\n",
      "  training loss (in-iteration): \t0.625662\n",
      "  validation accuracy: \t\t\t82.95 %\n",
      "Epoch 31 of 100 took 67.026s\n",
      "  training loss (in-iteration): \t0.613844\n",
      "  validation accuracy: \t\t\t83.32 %\n",
      "Epoch 32 of 100 took 67.387s\n",
      "  training loss (in-iteration): \t0.607634\n",
      "  validation accuracy: \t\t\t82.34 %\n",
      "Epoch 33 of 100 took 67.137s\n",
      "  training loss (in-iteration): \t0.598272\n",
      "  validation accuracy: \t\t\t83.32 %\n",
      "Epoch 34 of 100 took 67.547s\n",
      "  training loss (in-iteration): \t0.592070\n",
      "  validation accuracy: \t\t\t84.04 %\n",
      "Epoch 35 of 100 took 68.354s\n",
      "  training loss (in-iteration): \t0.596070\n",
      "  validation accuracy: \t\t\t83.61 %\n",
      "Epoch 36 of 100 took 69.784s\n",
      "  training loss (in-iteration): \t0.588450\n",
      "  validation accuracy: \t\t\t84.11 %\n",
      "Epoch 37 of 100 took 69.801s\n",
      "  training loss (in-iteration): \t0.578776\n",
      "  validation accuracy: \t\t\t83.47 %\n",
      "Epoch 38 of 100 took 68.411s\n",
      "  training loss (in-iteration): \t0.579986\n",
      "  validation accuracy: \t\t\t83.93 %\n",
      "Epoch 39 of 100 took 67.578s\n",
      "  training loss (in-iteration): \t0.571584\n",
      "  validation accuracy: \t\t\t84.07 %\n",
      "Epoch 40 of 100 took 69.661s\n",
      "  training loss (in-iteration): \t0.571911\n",
      "  validation accuracy: \t\t\t83.98 %\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 100 # total amount of full passes over training data\n",
    "\n",
    "early_stop = 0\n",
    "validation_acc_max = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    start_time = time.time()\n",
    "    model.train(True) # enable dropout / batch_norm training behavior\n",
    "    for (x_batch, y_batch) in train_batch_gen:\n",
    "        # train on batch\n",
    "        loss = compute_loss(x_batch, y_batch, device)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    # And a full pass over the validation data:\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    for (X_batch, y_batch) in valid_batch_gen:\n",
    "        logits = model(X_batch.to(device))\n",
    "        y_pred = logits.max(1)[1].data.cpu().numpy()\n",
    "        val_accuracy.append(np.mean(y_batch.data.numpy() == y_pred))\n",
    "\n",
    "        \n",
    "    validation_accuracy = np.mean(val_accuracy[-len(X_val) // batch_size :])\n",
    "    \n",
    "    if validation_accuracy > validation_acc_max:\n",
    "        validation_acc_max = validation_accuracy\n",
    "        early_stop = 0\n",
    "    else:\n",
    "        early_stop += 1\n",
    "        if early_stop >= 10:\n",
    "            break\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-len(X_train) // batch_size :])))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(validation_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at C:\\ProgramData\\Miniconda3\\conda-bld\\pytorch_1524549877902\\work\\torch/csrc/generic/serialization.cpp:17",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-d28538b6c18e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./model_aug_1.tml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torchEnv\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torchEnv\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[1;34m(f, mode, body)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torchEnv\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torchEnv\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         \u001b[0mserialized_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_is_real_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at C:\\ProgramData\\Miniconda3\\conda-bld\\pytorch_1524549877902\\work\\torch/csrc/generic/serialization.cpp:17"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"./model_aug_1.tml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524549877902\\work\\aten\\src\\thc\\generic/THCTensorCopy.c:20",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-56a2e9e5d0ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_batch_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_batch_gen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtest_batch_acc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524549877902\\work\\aten\\src\\thc\\generic/THCTensorCopy.c:20"
     ]
    }
   ],
   "source": [
    "#model.to(torch.device(\"cpu\"))\n",
    "model.train(False) # disable dropout / use averages for batch_norm\n",
    "test_batch_acc = []\n",
    "for (X_batch, y_batch) in test_batch_gen:\n",
    "    logits = model(X_batch.to(device))\n",
    "    y_pred = logits.max(1)[1].data.numpy()\n",
    "    test_batch_acc.append(np.mean(y_batch.data.numpy() == y_pred))\n",
    "\n",
    "test_accuracy = np.mean(test_batch_acc)\n",
    "    \n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_accuracy * 100))\n",
    "\n",
    "if test_accuracy * 100 > 95:\n",
    "    print(\"Double-check, than consider applying for NIPS'17. SRSly.\")\n",
    "elif test_accuracy * 100 > 90:\n",
    "    print(\"U'r freakin' amazin'!\")\n",
    "elif test_accuracy * 100 > 80:\n",
    "    print(\"Achievement unlocked: 110lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 70:\n",
    "    print(\"Achievement unlocked: 80lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 60:\n",
    "    print(\"Achievement unlocked: 70lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 50:\n",
    "    print(\"Achievement unlocked: 60lvl Warlock!\")\n",
    "else:\n",
    "    print(\"We need more magic! Follow instructons below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# Report\n",
    "\n",
    "All creative approaches are highly welcome, but at the very least it would be great to mention\n",
    "* the idea;\n",
    "* brief history of tweaks and improvements;\n",
    "* what is the final architecture and why?\n",
    "* what is the training method and, again, why?\n",
    "* Any regularizations and other techniques applied and their effects;\n",
    "\n",
    "\n",
    "There is no need to write strict mathematical proofs (unless you want to).\n",
    " * \"I tried this, this and this, and the second one turned out to be better. And i just didn't like the name of that one\" - OK, but can be better\n",
    " * \"I have analized these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\" - the ideal one\n",
    " * \"I took that code that demo without understanding it, but i'll never confess that and instead i'll make up some pseudoscientific explaination\" - __not_ok__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi, my name is `___ ___`, and here's my story\n",
    "\n",
    "A long time ago in a galaxy far far away, when it was still more than an hour before the deadline, i got an idea:\n",
    "\n",
    "##### I gonna build a neural network, that\n",
    "* brief text on what was\n",
    "* the original idea\n",
    "* and why it was so\n",
    "\n",
    "How could i be so naive?!\n",
    "\n",
    "##### One day, with no signs of warning,\n",
    "This thing has finally converged and\n",
    "* Some explaination about what were the results,\n",
    "* what worked and what didn't\n",
    "* most importantly - what next steps were taken, if any\n",
    "* and what were their respective outcomes\n",
    "\n",
    "##### Finally, after __  iterations, __ mugs of [tea/coffee]\n",
    "* what was the final architecture\n",
    "* as well as training method and tricks\n",
    "\n",
    "That, having wasted ____ [minutes, hours or days] of my life training, got\n",
    "\n",
    "* accuracy on training: __\n",
    "* accuracy on validation: __\n",
    "* accuracy on test: __\n",
    "\n",
    "\n",
    "[an optional afterword and mortal curses on assignment authors]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
